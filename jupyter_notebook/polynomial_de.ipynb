{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Polynomial Dataset Dimension Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder innermost layer is refashioned into singular value proxies (SVP).  These SVP are used to estimate dimension of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What is Synthetic Polynomial? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin out experiments by considering synthetic polynomial $f(x,y,z)$, where $f(x,y,z)$ comprises $3^{rd}$ degree, $2^{nd}$ degree, $1^{st}$ terms and cross terms of $x, y, $ and $z$.\n",
    "\n",
    "\\begin{equation}\n",
    "  \\begin{aligned}\n",
    "    f(x,y,z) &= c_0x^3 + c_1x^2y + c_2x^2z + c_3x^2 + c_4xy^2 \\\\ \n",
    "     &\\qquad + c_5xyz + c_6xy + c_7xz^2 + c_8xz + c_9x\\\\\n",
    "     &\\qquad + c_{10}y^3 + c_{11}y^2z + c_{12}y^2 + c_{13}yz^2 + c_{14}yz\\\\ \n",
    "     &\\qquad + c_{15}y + c_{16}z^3 + c_{17}z^2 + c_{18}z + c_{k}\n",
    "  \\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The coefficients $c_0, c_1, c_2, c_3, c_4, ..., c_{17}, c_{18}$, and independent variables $x, y, z$ are randomly generated. \n",
    "\n",
    "Random numbers are generated from $c_k \\sim \\mathcal{N} (\\mu=0,\\sigma^{2}=1.0)$, where $k \\in [0, 18]$ and $-1 < c_k < 1$. \n",
    "\n",
    "We generate $5000$ tuples of $x, y, z$, a tuple for each row, and $784$ tuples of $c_0, c_1, c_2, c_3, c_4, ..., c_{17}, c_{18}$, a tuple for each column. $f(x,y,z)$ non-linearly maps the $3$ dimensional data to $784$ dimensions. Because the coefficients are randomly generated, we call $f(x,y,z)$ <i>randomly generated taylor polynomial</i>. \n",
    "\n",
    "We $DE(\\boldsymbol{X})$, where $\\boldsymbol{X} \\in\\mathbb {R}^{5000 \\times 784}$. \n",
    "\n",
    "The first $3000$ instances of $\\boldsymbol{X}$ forms the training set and the remaining $2000$ instances form the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.regularizers import Regularizer\n",
    "\n",
    "import scipy.sparse\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iPython notebook code is only being provided for convenience.  \n",
    "\n",
    "All Linux code along with scripts is available at https://github.com/nitishbahadur/book_chapter. Our Linux code is based on tensorflow 1.x.  Python package requirements were exported and available https://github.com/nitishbahadur/book_chapter/blob/master/src/requirements.txt.\n",
    "\n",
    "We run our production code on https://arc.wpi.edu/cluster-documentation/build/html/clusters.html for performance reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.get_default_graph()\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The synthetic polynomial data is loaded from data folder. We treat the first $3000$ instances as training dataset and remaining $2000$ instances as test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_input_folder = r'../data/synthetic_polynomial/input'\n",
    "base_output_folder = r'../data/synthetic_polynomial/output'\n",
    "input_filename = r'Y_{}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_filename(std_dev):\n",
    "    filename = input_filename.format(std_dev)\n",
    "    filepath = os.path.join(base_input_folder, filename)\n",
    "    return filepath\n",
    "\n",
    "def get_synthetic_data(std_dev):\n",
    "    filepath = get_input_filename(std_dev)\n",
    "    print(f\"get_synthetic_data is loading {filepath}\")\n",
    "    df = pd.read_csv(filepath, header=None)\n",
    "    X = df.values\n",
    "    X = X.astype('float32')\n",
    "    X = X / (np.max(X) - np.min(X))\n",
    "    x_train = X[0:3000,:]\n",
    "    x_test = X[3000:,:]\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the autoencoder model where the innermost layer is using a sigmoid activation function.  The autoencoder also uses dropout layers to control for overfitting.  We use a custom loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lite_ae_model(l1_reg, encoding_dim, layer1_dropout, layer2_dropout, layer3_dropout):\n",
    "    input_img = Input(shape=(784,))\n",
    "    encoded = Dense(392, activation='relu')(input_img)\n",
    "    encoded = Dropout(layer1_dropout)(encoded)\n",
    "    encoded = Dense(128, activation='relu')(encoded)\n",
    "    encoded = Dropout(layer2_dropout)(encoded)\n",
    "\n",
    "    # an additional layer with 30% dropout added to encoder\n",
    "#     encoded = Dense(64, activation='relu')(encoded)\n",
    "#     encoded = Dropout(layer3_dropout)(encoded)\n",
    "\n",
    "    z_layer_input = Lambda(lambda  x: K.l2_normalize(x,axis=1))(encoded)\n",
    "    encoded = Dense(encoding_dim, activation='sigmoid')(z_layer_input)\n",
    "    encoded_norm = Lambda(lambda  x: K.l2_normalize(x,axis=1))(encoded)\n",
    "    \n",
    "    # an additional layer to match the encoder is added to decoder\n",
    "#     decoded = Dense(64, activation='relu')(encoded)\n",
    "\n",
    "    # was in the original ICMLA AEDE\n",
    "    decoded = Dense(128, activation='relu')(encoded)\n",
    "#     decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dense(392, activation='relu')(decoded)\n",
    "    decoded = Dense(784, activation='tanh')(decoded)\n",
    "\n",
    "    # create autoencoder\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "\n",
    "    # create encoder\n",
    "    encoder = Model(input_img, encoded)\n",
    "\n",
    "    # create decoder model\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "#     deco = autoencoder.layers[-4](encoded_input) # new code to match additional 64 layer encoder/decoder\n",
    "    deco = autoencoder.layers[-3](encoded_input) # new code to match additional 64 layer encoder/decoder\n",
    "#     deco = autoencoder.layers[-3](deco)\n",
    "    deco = autoencoder.layers[-2](deco)\n",
    "    deco = autoencoder.layers[-1](deco)\n",
    "    decoder = Model(encoded_input, deco)    \n",
    "\n",
    "    # autoencoder.compile(optimizer='adadelta', loss='mse') \n",
    "    autoencoder.compile(optimizer='adadelta', loss=mse_regularized_loss(encoded_norm, l1_reg)) \n",
    "    return encoder, decoder, autoencoder\n",
    "\n",
    "\n",
    "def mse_regularized_loss(encoded_layer, lambda_):    \n",
    "    def loss(y_true, y_pred):\n",
    "        return K.mean(K.square(y_pred - y_true) + lambda_ * K.sum(K.abs(encoded_layer)))\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility functions provided below is equivalent to the python code we use on HPC cluster.  We provide this for completeness here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_model_path(output_file_type, encoding_dim, l1_reg):\n",
    "    filename = r'{}_l1_reg_{}_{}.h5'.format(output_file_type, encoding_dim, l1_reg)\n",
    "    filepath = os.path.join(base_output_folder, filename)\n",
    "    return filepath\n",
    "\n",
    "def save_model(encoding_dim, l1_reg, autoencoder, encoder, decoder):\n",
    "    autoencoder_model_path = get_output_model_path(\"autoencoder\", encoding_dim, l1_reg)\n",
    "    encoder_model_path = get_output_model_path(\"encoder\", encoding_dim, l1_reg)\n",
    "    decoder_model_path = get_output_model_path(\"decoder\", encoding_dim, l1_reg)\n",
    "\n",
    "    autoencoder.save(autoencoder_model_path)\n",
    "    print(\"autoencoder saved!!!\")\n",
    "\n",
    "    encoder.save(encoder_model_path) \n",
    "    print(\"encoder saved!!!\")\n",
    "\n",
    "    decoder.save(decoder_model_path) \n",
    "    print(\"decoder saved!!!\")\n",
    "\n",
    "def save_history(encoding_dim, l1_reg, history):\n",
    "    history_filename = get_output_model_path(\"history\", encoding_dim, l1_reg)\n",
    "    with open(history_filename, 'w') as f:\n",
    "        json.dump(history.history, f)\n",
    "\n",
    "def save_intermediate_training(x, encoder, decoder, epoch):\n",
    "    input_type = 'train'\n",
    "    x_encoded = encoder.predict(x)\n",
    "    x_reconstructed = decoder.predict(x_encoded)\n",
    "\n",
    "    x_encoded_filename = r\"../data/synthetic_polynomial/output/x_{}_{}_encoded_{}_{}\"\n",
    "    np.save(x_encoded_filename.format(input_type, epoch, encoding_dim, l1_reg), x_encoded)\n",
    "\n",
    "    x_reconstructed_filename = r\"../data/synthetic_polynomial/output/x_{}_{}_reconstructed_{}_{}\"\n",
    "    np.save(x_reconstructed_filename.format(input_type, epoch, encoding_dim, l1_reg), x_reconstructed)\n",
    "\n",
    "\n",
    "def save_output(x, autoencoder, encoder, decoder, layer1_dropout, layer2_dropout, layer3_dropout, input_type):\n",
    "    print(\"{} Original : \".format(input_type))\n",
    "    print(x)\n",
    "\n",
    "    print(\"{} Predicted : \".format(input_type))\n",
    "    x_predicted = autoencoder.predict(x)\n",
    "    print(x_predicted)\n",
    "\n",
    "    print(\"{} Original->Encoded->Decoded(Reconsturcted) : \".format(input_type))\n",
    "    x_encoded = encoder.predict(x)\n",
    "    x_reconstructed = decoder.predict(x_encoded)\n",
    "    print(x_reconstructed)\n",
    "\n",
    "    print(\"{} Encoded : \".format(input_type))\n",
    "    print(x_encoded)    \n",
    "\n",
    "    x_filename = r\"../data/synthetic_polynomial/output/x_{}_{}_{}_{}_{}_{}\"\n",
    "    np.save(x_filename.format(input_type, encoding_dim, l1_reg, layer1_dropout, layer2_dropout, layer3_dropout), x)\n",
    "\n",
    "    x_encoded_filename = r\"../data/synthetic_polynomial/output/x_{}_encoded_{}_{}_{}_{}_{}\"\n",
    "    np.save(x_encoded_filename.format(input_type, encoding_dim, l1_reg, layer1_dropout, layer2_dropout, layer3_dropout), x_encoded)\n",
    "\n",
    "    x_predicted_filename = r\"../data/synthetic_polynomial/output/x_{}_predicted_{}_{}_{}_{}_{}\"\n",
    "    np.save(x_predicted_filename.format(input_type, encoding_dim, l1_reg, layer1_dropout, layer2_dropout, layer3_dropout), x_predicted)\n",
    "\n",
    "class SaveIntermediateTrainingOutput(Callback):\n",
    "    def __init__(self, x, encoder, decoder):\n",
    "        super(Callback, self).__init__()\n",
    "        self.x = x\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.counter = 1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"File counter: {}\".format(self.counter*(epoch+1)))\n",
    "            save_intermediate_training(self.x, self.encoder, self.decoder, self.counter*(epoch+1))\n",
    "            self.counter = self.counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate dimension by counting how many singular value proxies are greater than 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_gt_threshold(z, threshold):\n",
    "    tot = sum(z)\n",
    "    z_pct = [(i/tot) for i in sorted(z, reverse=True)]\n",
    "    z_gt_theta = [i for i in z_pct if i >= threshold]\n",
    "    return len(z_gt_theta)\n",
    "\n",
    "def sort_by_row(z):\n",
    "    z_sorted = None\n",
    "    for i in np.arange(z.shape[0]):\n",
    "        z_s = sorted(z[i,:], reverse=True)\n",
    "        if z_sorted is None:\n",
    "            z_sorted = z_s\n",
    "        else:\n",
    "            z_sorted = np.vstack((z_sorted,z_s))\n",
    "    return z_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience we provide default values from run_polynomial_de.sh script.  The script is used to run DE process on High Performance Computing cluster at WPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparsity parameter\n",
    "l1_reg = 3e-2\n",
    "\n",
    "# number of nodes in innermost hidden layer\n",
    "encoding_dim = 16\n",
    "\n",
    "# number of times you want to run 100 epochs\n",
    "# DE converges slowly.\n",
    "num_epochs = 40\n",
    "\n",
    "# the batch size\n",
    "batch_size = 64\n",
    "\n",
    "# 30% of nodes are dropped out\n",
    "layer1_dropout = 0.3\n",
    "\n",
    "# 30% of nodes are dropped out\n",
    "layer2_dropout = 0.3\n",
    "\n",
    "# 30% of nodes are dropped out\n",
    "layer3_dropout = 0.3\n",
    "\n",
    "# the input file type to be loaded\n",
    "std_dev = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_synthetic_data is loading ../data/synthetic_polynomial/input\\Y_1.0.csv\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test = get_synthetic_data(std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running standard AE with the following parameters : \n",
      "x_train dimension : (3000 x 784)\n",
      "encoding_dim dimension : 16\n",
      "epochs : 4000 batch_size : 64\n",
      "layer1_dropout : 0.3 layer2_dropout : 0.3 layer3_dropout : 0.3\n",
      "std_dev : 1.0\n",
      "Running encoding_dim: 16 l1_reg: 0.03\n"
     ]
    }
   ],
   "source": [
    "print(\"Running standard AE with the following parameters : \")\n",
    "print(\"x_train dimension : ({} x {})\".format(x_train.shape[0], x_train.shape[1]))\n",
    "print(\"encoding_dim dimension : {}\".format(encoding_dim))\n",
    "print(\"epochs : {} batch_size : {}\".format(num_epochs*100, batch_size))\n",
    "print(\"layer1_dropout : {} layer2_dropout : {} layer3_dropout : {}\".format(layer1_dropout, layer2_dropout, layer3_dropout))\n",
    "print(\"std_dev : {}\".format(std_dev))\n",
    "print(\"Running encoding_dim: {} l1_reg: {}\".format(encoding_dim, l1_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the encoder, decoder, and autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder, autoencoder = build_lite_ae_model(l1_reg, encoding_dim, layer1_dropout, layer2_dropout, layer3_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE,1.0,100,7.5851,16,16\n",
      "0.6118,0.5994,0.5948,0.5804,0.5595,0.5294,0.5235,0.5217,0.5112,0.4635,0.462,0.4135,0.4024,0.3937,0.375,0.3371\n",
      "\n",
      "AE,1.0,200,7.4694,16,16\n",
      "0.6558,0.6527,0.6487,0.6392,0.5935,0.5612,0.5342,0.5195,0.5133,0.4155,0.4086,0.385,0.3304,0.3202,0.3049,0.2879\n",
      "\n",
      "AE,1.0,300,7.2714,16,16\n",
      "0.7095,0.7041,0.6981,0.6957,0.6326,0.596,0.538,0.5119,0.5002,0.3481,0.3355,0.3295,0.2726,0.2403,0.2359,0.2254\n",
      "\n",
      "AE,1.0,400,6.9928,16,16\n",
      "0.7632,0.7542,0.7506,0.742,0.6757,0.6334,0.5229,0.4792,0.4636,0.265,0.2631,0.2497,0.214,0.1868,0.1721,0.1608\n",
      "\n",
      "AE,1.0,500,6.6665,16,16\n",
      "0.8073,0.8012,0.7904,0.7821,0.7191,0.6701,0.4734,0.4076,0.3889,0.2003,0.1885,0.1746,0.1641,0.146,0.1225,0.1147\n",
      "\n",
      "AE,1.0,600,6.3144,16,16\n",
      "0.84,0.8381,0.8221,0.8145,0.7577,0.7027,0.3788,0.3035,0.2846,0.1507,0.1332,0.1253,0.1224,0.1141,0.0896,0.084\n",
      "\n",
      "AE,1.0,700,5.9607,16,16\n",
      "0.8646,0.8644,0.8466,0.8395,0.7886,0.7283,0.2607,0.2039,0.1901,0.114,0.0967,0.0965,0.0901,0.0883,0.0677,0.0636\n",
      "\n",
      "AE,1.0,800,5.6627,14,14\n",
      "0.8832,0.8827,0.8654,0.8579,0.8106,0.7436,0.167,0.1353,0.127,0.0873,0.0756,0.0723,0.0718,0.0657,0.0527,0.0497\n",
      "\n",
      "AE,1.0,900,5.4447,11,11\n",
      "0.8967,0.8962,0.8796,0.872,0.8268,0.7497,0.1109,0.0939,0.0893,0.0684,0.0601,0.0578,0.0557,0.0508,0.0419,0.0397\n",
      "\n",
      "AE,1.0,1000,5.2920,9,9\n",
      "0.907,0.9064,0.8905,0.8829,0.8387,0.745,0.0783,0.0686,0.066,0.0546,0.0486,0.0471,0.044,0.0403,0.0341,0.0323\n",
      "\n",
      "AE,1.0,1100,5.1766,7,7\n",
      "0.915,0.9146,0.8992,0.8916,0.8479,0.7276,0.0581,0.0523,0.0507,0.0443,0.0399,0.039,0.0357,0.0328,0.0282,0.0269\n",
      "\n",
      "AE,1.0,1200,5.0884,6,6\n",
      "0.9216,0.9214,0.9066,0.8989,0.8554,0.692,0.0448,0.0411,0.0401,0.0366,0.0331,0.0326,0.0295,0.0272,0.0239,0.0227\n",
      "\n",
      "AE,1.0,1300,5.0130,6,6\n",
      "0.9273,0.9273,0.9131,0.905,0.8617,0.6255,0.0356,0.0332,0.0326,0.0307,0.0279,0.0277,0.0248,0.023,0.0205,0.0194\n",
      "\n",
      "AE,1.0,1400,4.9327,6,6\n",
      "0.9326,0.932,0.9186,0.9098,0.8672,0.5048,0.0289,0.0275,0.0272,0.0262,0.024,0.0239,0.0213,0.0198,0.0181,0.0169\n",
      "\n",
      "AE,1.0,1500,4.8230,6,6\n",
      "0.9369,0.9359,0.9233,0.9134,0.8716,0.3314,0.0241,0.0233,0.0233,0.0228,0.0213,0.0208,0.0187,0.0174,0.0162,0.015\n",
      "\n",
      "AE,1.0,1600,4.7047,6,6\n",
      "0.9406,0.9395,0.9276,0.9176,0.8753,0.1909,0.0205,0.02,0.0199,0.0198,0.0186,0.0181,0.0164,0.0153,0.0144,0.0133\n",
      "\n",
      "AE,1.0,1700,4.6162,6,6\n",
      "0.9438,0.9428,0.9315,0.9219,0.878,0.1161,0.0176,0.0172,0.0171,0.0171,0.0161,0.0158,0.0144,0.0134,0.0127,0.0118\n",
      "\n",
      "AE,1.0,1800,4.5581,6,6\n",
      "0.9465,0.9454,0.9349,0.9257,0.8787,0.0774,0.0153,0.015,0.015,0.0149,0.014,0.0139,0.0127,0.0119,0.0113,0.0106\n",
      "\n",
      "AE,1.0,1900,4.5170,6,6\n",
      "0.9487,0.9477,0.9375,0.9286,0.8784,0.0556,0.0135,0.0132,0.0132,0.0131,0.0124,0.0123,0.0113,0.0107,0.0101,0.0096\n",
      "\n",
      "AE,1.0,2000,4.4871,5,5\n",
      "0.9505,0.9497,0.9397,0.9309,0.8768,0.042,0.012,0.0118,0.0117,0.0117,0.0111,0.011,0.0102,0.0096,0.0091,0.0087\n",
      "\n",
      "AE,1.0,2100,4.4638,5,5\n",
      "0.9522,0.9514,0.9415,0.9329,0.874,0.033,0.0108,0.0106,0.0105,0.0105,0.0099,0.0099,0.0092,0.0088,0.0083,0.0079\n",
      "\n",
      "AE,1.0,2200,4.4441,5,5\n",
      "0.9536,0.9529,0.9432,0.9346,0.8698,0.0268,0.0098,0.0096,0.0095,0.0095,0.009,0.009,0.0084,0.008,0.0076,0.0073\n",
      "\n",
      "AE,1.0,2300,4.4286,5,5\n",
      "0.9549,0.9543,0.9446,0.9362,0.8641,0.0222,0.0089,0.0087,0.0087,0.0086,0.0081,0.0081,0.0077,0.0074,0.0069,0.0067\n",
      "\n",
      "AE,1.0,2400,4.4156,5,5\n",
      "0.9562,0.9555,0.946,0.9376,0.8564,0.0189,0.0082,0.008,0.008,0.0079,0.0074,0.0074,0.0071,0.0068,0.0064,0.0062\n",
      "\n",
      "AE,1.0,2500,4.4030,5,5\n",
      "0.9574,0.9567,0.9473,0.9388,0.8459,0.0163,0.0075,0.0073,0.0073,0.0072,0.0068,0.0068,0.0065,0.0063,0.0059,0.0058\n",
      "\n",
      "AE,1.0,2600,4.3925,5,5\n",
      "0.9585,0.9578,0.9485,0.9401,0.8314,0.0143,0.0069,0.0068,0.0068,0.0067,0.0063,0.0063,0.0061,0.0059,0.0055,0.0054\n",
      "\n",
      "AE,1.0,2700,4.3821,5,5\n",
      "0.9596,0.959,0.9496,0.9413,0.8105,0.0126,0.0064,0.0063,0.0063,0.0062,0.0058,0.0058,0.0056,0.0055,0.0052,0.0051\n",
      "\n",
      "AE,1.0,2800,4.3705,5,5\n",
      "0.9606,0.96,0.9507,0.9425,0.7776,0.0113,0.006,0.0059,0.0059,0.0058,0.0054,0.0054,0.0053,0.0051,0.0049,0.0048\n",
      "\n",
      "AE,1.0,2900,4.3555,5,5\n",
      "0.9615,0.9608,0.9515,0.9433,0.7214,0.0102,0.0057,0.0055,0.0055,0.0055,0.0051,0.005,0.005,0.0049,0.0046,0.0046\n",
      "\n",
      "AE,1.0,3000,4.3262,5,5\n",
      "0.962,0.9611,0.9515,0.9432,0.612,0.0094,0.0056,0.0055,0.0054,0.0054,0.005,0.0049,0.0048,0.0048,0.0046,0.0045\n",
      "\n",
      "AE,1.0,3100,4.2513,5,5\n",
      "0.9607,0.9593,0.9492,0.9405,0.4051,0.0089,0.006,0.0058,0.0057,0.0056,0.0053,0.0052,0.0051,0.0051,0.005,0.0048\n",
      "\n",
      "AE,1.0,3200,4.1346,5,5\n",
      "0.9602,0.9586,0.9486,0.9397,0.2124,0.0084,0.0061,0.0058,0.0057,0.0055,0.0053,0.0053,0.0052,0.0051,0.005,0.0049\n",
      "\n",
      "AE,1.0,3300,4.0512,5,5\n",
      "0.9622,0.9608,0.9514,0.9426,0.1212,0.0075,0.0053,0.0052,0.0051,0.005,0.0048,0.0047,0.0047,0.0046,0.0045,0.0044\n",
      "\n",
      "AE,1.0,3400,4.0013,5,5\n",
      "0.9641,0.9629,0.9541,0.9455,0.0791,0.0068,0.0046,0.0046,0.0045,0.0044,0.0042,0.0042,0.0041,0.0041,0.004,0.004\n",
      "\n",
      "AE,1.0,3500,3.9725,5,5\n",
      "0.9655,0.9645,0.9561,0.9475,0.0563,0.0062,0.0042,0.0041,0.0041,0.004,0.0038,0.0037,0.0037,0.0037,0.0037,0.0036\n",
      "\n",
      "AE,1.0,3600,3.9517,5,5\n",
      "0.9666,0.9656,0.9575,0.949,0.0425,0.0057,0.0039,0.0038,0.0037,0.0037,0.0035,0.0035,0.0034,0.0034,0.0034,0.0033\n",
      "\n",
      "AE,1.0,3700,3.9377,4,4\n",
      "0.9674,0.9665,0.9585,0.95,0.0333,0.0053,0.0036,0.0036,0.0035,0.0034,0.0032,0.0032,0.0032,0.0031,0.0031,0.0031\n",
      "\n",
      "AE,1.0,3800,3.9270,4,4\n",
      "0.968,0.9673,0.9593,0.9508,0.027,0.005,0.0034,0.0033,0.0033,0.0032,0.0031,0.003,0.003,0.003,0.003,0.0029\n",
      "\n",
      "AE,1.0,3900,3.9190,4,4\n",
      "0.9686,0.9678,0.96,0.9513,0.0224,0.0047,0.0032,0.0032,0.0031,0.003,0.0029,0.0029,0.0028,0.0028,0.0028,0.0028\n",
      "\n",
      "AE,1.0,4000,3.9126,4,4\n",
      "0.969,0.9683,0.9605,0.9518,0.019,0.0044,0.0031,0.003,0.003,0.0028,0.0028,0.0027,0.0027,0.0027,0.0027,0.0027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svp_dict_ = {}\n",
    "dim_dict_ = {}\n",
    "for i in range(1, num_epochs+1):\n",
    "        history = autoencoder.fit(x_train, x_train, epochs=100, batch_size=batch_size, verbose=0)\n",
    "        z = encoder.predict(x_test) # use x_test\n",
    "        z_row_sorted = sort_by_row(z)\n",
    "        z_mu = np.mean(z_row_sorted, axis=0)\n",
    "        gte_sorted = count_gt_threshold(z_mu, 0.01)\n",
    "        \n",
    "        z_mu_1 = sorted(np.mean(z, axis=0), reverse=True)\n",
    "        gte_dim = count_gt_threshold(z_mu_1, 0.01)\n",
    "        loss = history.history['loss'][-1]\n",
    "                \n",
    "        print(\"AE,{},{},{:.4f},{},{}\".format(std_dev, i*100, loss, gte_sorted, gte_dim))\n",
    "        \n",
    "        converted_list = [str(np.round(element, 4)) for element in z_mu_1]\n",
    "        svps = \",\".join(converted_list)    \n",
    "        print(svps)\n",
    "        print()\n",
    "        \n",
    "        # save it for plotting later\n",
    "        svp_dict_[i*100] = svps\n",
    "        dim_dict_[i*100] = gte_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The singular value proxy keeps reducing.  Notice the values on the left are increasing and the values on the right are decreasing.  We take a snapshot every 100 epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythongpu",
   "language": "python",
   "name": "pythongpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
