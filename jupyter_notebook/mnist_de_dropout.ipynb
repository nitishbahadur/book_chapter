{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset Dimension Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder innermost layer is refashioned into singular value proxies (SVP).  These SVP are used to estimate dimension of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.regularizers import Regularizer\n",
    "\n",
    "import scipy.sparse\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iPython notebook code is only being provided for convenience.  \n",
    "\n",
    "All Linux code along with scripts is available at https://github.com/nitishbahadur/book_chapter. Our Linux code is based on tensorflow 1.x.  Python package requirements were exported and available https://github.com/nitishbahadur/book_chapter/blob/master/src/requirements.txt.\n",
    "\n",
    "We run our production code on https://arc.wpi.edu/cluster-documentation/build/html/clusters.html for performance reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.get_default_graph()\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST data is loaded from data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_data():\n",
    "    file_path = r'../data/mnist/input/mnist_x_train.npy'\n",
    "    x_train = np.load(file_path)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_train = x_train/np.max(x_train)\n",
    "    \n",
    "    file_path = r'../data/mnist/input/mnist_x_test.npy'\n",
    "    x_test = np.load(file_path)\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_test = x_test/np.max(x_test)\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the autoencoder model where the innermost layer is using a sigmoid activation function.  The autoencoder also uses dropout layers to control for overfitting.  We use a custom loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lite_ae_model(l1_reg, encoding_dim, layer1_dropout, layer2_dropout):\n",
    "    input_img = Input(shape=(784,))\n",
    "    encoded = Dense(392, activation='relu')(input_img)\n",
    "    encoded = Dropout(layer1_dropout)(encoded)\n",
    "    encoded = Dense(128, activation='relu')(encoded)\n",
    "    encoded = Dropout(layer2_dropout)(encoded)\n",
    "\n",
    "    z_layer_input = Lambda(lambda  x: K.l2_normalize(x,axis=1))(encoded)\n",
    "    encoded = Dense(encoding_dim, activation='sigmoid')(z_layer_input)\n",
    "    encoded_norm = Lambda(lambda  x: K.l2_normalize(x,axis=1))(encoded)\n",
    "    \n",
    "    decoded = Dense(128, activation='relu')(encoded)\n",
    "    decoded = Dense(392, activation='relu')(decoded)\n",
    "    decoded = Dense(784, activation='sigmoid')(decoded)\n",
    "\n",
    "    # create autoencoder\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "\n",
    "    # create encoder\n",
    "    encoder = Model(input_img, encoded)\n",
    "\n",
    "    # create decoder model\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    deco = autoencoder.layers[-3](encoded_input)\n",
    "    deco = autoencoder.layers[-2](deco)\n",
    "    deco = autoencoder.layers[-1](deco)\n",
    "    decoder = Model(encoded_input, deco)    \n",
    "\n",
    "    autoencoder.compile(optimizer='adadelta', loss=mse_regularized_loss(encoded_norm, l1_reg)) \n",
    "    return encoder, decoder, autoencoder\n",
    "\n",
    "\n",
    "def mse_regularized_loss(encoded_norm, lambda_):    \n",
    "    def loss(y_true, y_pred):\n",
    "        return K.mean(K.square(y_pred - y_true) + lambda_ * K.sum(K.abs(encoded_norm)))\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility functions provided below is equivalent to the python code we use on HPC cluster.  We provide this for completeness here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(x, autoencoder, encoder, decoder, layer1_dropout, layer2_dropout, input_type):\n",
    "    print(\"{} Original : \".format(input_type))\n",
    "    print(x)\n",
    "\n",
    "    print(\"{} Predicted : \".format(input_type))\n",
    "    x_predicted = autoencoder.predict(x)\n",
    "    print(x_predicted)\n",
    "\n",
    "    print(\"{} Original->Encoded->Decoded(Reconsturcted) : \".format(input_type))\n",
    "    x_encoded = encoder.predict(x)\n",
    "    x_reconstructed = decoder.predict(x_encoded)\n",
    "    print(x_reconstructed)\n",
    "\n",
    "    print(\"{} Encoded : \".format(input_type))\n",
    "    print(x_encoded)    \n",
    "\n",
    "    x_filename = r\"../data/mnist/output/mnist_x_{}_{}_{}_{}_{}\"\n",
    "    np.save(x_filename.format(input_type, encoding_dim, l1_reg, layer1_dropout, layer2_dropout), x)\n",
    "\n",
    "    x_encoded_filename = r\"../data/mnist/output/mnist_x_{}_encoded_{}_{}_{}_{}\"\n",
    "    np.save(x_encoded_filename.format(input_type, encoding_dim, l1_reg, layer1_dropout, layer2_dropout), x_encoded)\n",
    "\n",
    "    x_predicted_filename = r\"../data/mnist/output/mnist_x_{}_predicted_{}_{}_{}_{}\"\n",
    "    np.save(x_predicted_filename.format(input_type, encoding_dim, l1_reg, layer1_dropout, layer2_dropout), x_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate dimension by counting how many singular value proxies are greater than 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_gt_threshold(z, threshold):\n",
    "    tot = sum(z)\n",
    "    z_pct = [(i/tot) for i in sorted(z, reverse=True)]\n",
    "    z_gt_theta = [i for i in z_pct if i >= threshold]\n",
    "    return len(z_gt_theta)\n",
    "\n",
    "def sort_by_row(z):\n",
    "    z_sorted = None\n",
    "    for i in np.arange(z.shape[0]):\n",
    "        z_s = sorted(z[i,:], reverse=True)\n",
    "        if z_sorted is None:\n",
    "            z_sorted = z_s\n",
    "        else:\n",
    "            z_sorted = np.vstack((z_sorted,z_s))\n",
    "    return z_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience we provide default values from run_synthetic_de.py script.  The script is used to run DE process on High Performance Computing cluster at WPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparsity parameter\n",
    "l1_reg = 5e-5\n",
    "\n",
    "# number of nodes in innermost hidden layer\n",
    "encoding_dim = 64\n",
    "\n",
    "# number of times you want to run 500 epochs\n",
    "# DE converges slowly.\n",
    "num_epochs = 30\n",
    "\n",
    "# the batch size\n",
    "batch_size = 64\n",
    "\n",
    "# 20% of nodes are dropped out\n",
    "layer1_dropout = 0.2\n",
    "\n",
    "# 20% of nodes are dropped out\n",
    "layer2_dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = get_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running standard AE with the following parameters : \n",
      "x_train dimension : (60000 x 784)\n",
      "encoding_dim dimension : 64\n",
      "epochs : 15000 batch_size : 64\n",
      "layer1_dropout : 0.2 layer2_dropout : 0.2\n",
      "Running encoding_dim: 64 l1_reg: 5e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"Running standard AE with the following parameters : \")\n",
    "print(\"x_train dimension : ({} x {})\".format(x_train.shape[0], x_train.shape[1]))\n",
    "print(\"encoding_dim dimension : {}\".format(encoding_dim))\n",
    "print(\"epochs : {} batch_size : {}\".format((num_epochs)*500, batch_size))\n",
    "print(\"layer1_dropout : {} layer2_dropout : {}\".format(layer1_dropout, layer2_dropout))\n",
    "print(\"Running encoding_dim: {} l1_reg: {}\".format(encoding_dim, l1_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the encoder, decoder, and autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder, autoencoder = build_lite_ae_model(l1_reg, encoding_dim, layer1_dropout, layer2_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "svp_dict_ = {}\n",
    "dim_dict_ = {}\n",
    "for i in range(1, num_epochs+1):\n",
    "        history = autoencoder.fit(x_train, x_train, epochs=500, batch_size=batch_size, verbose=0)\n",
    "        z = encoder.predict(x_test)\n",
    "        z_row_sorted = sort_by_row(z)\n",
    "        z_mu = np.mean(z_row_sorted, axis=0)\n",
    "        gte_sorted = count_gt_threshold(z_mu, 0.01)\n",
    "        \n",
    "        z_mu_1 = sorted(np.mean(z, axis=0), reverse=True)\n",
    "        gte_dim = count_gt_threshold(z_mu_1, 0.01)\n",
    "        loss = history.history['loss'][-1]\n",
    "        print(\"AE,{},{:.4f},{},{}\".format(i*500, loss, gte_sorted, gte_dim))\n",
    "        \n",
    "        converted_list = [str(np.round(element, 4)) for element in z_mu_1]\n",
    "        svps = \",\".join(converted_list)    \n",
    "        print(svps)\n",
    "        print()\n",
    "        \n",
    "        # save it for plotting later\n",
    "        svp_dict_[i*500] = svps\n",
    "        dim_dict_[i*500] = gte_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythongpu",
   "language": "python",
   "name": "pythongpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
