{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset Dimension Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder innermost layer is refashioned into singular value proxies (SVP).  These SVP are used to estimate dimension of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we create the sythetic dataset?\n",
    "\n",
    "We created a random array $\\mathbf{C} = \\mathbf{A} \\times \\mathbf{B}$ \n",
    " - $\\mathbf{A}~\\epsilon~\\mathbb{R}^{5000\\times5}$\n",
    " - $\\mathbf{B}~\\epsilon~\\mathbb{R}^{5\\times784}$\n",
    " \n",
    "We apply nonlinear transformations to $\\mathbf{C}$ to create $\\mathbf{D} = g(\\mathbf{C})$, where $\\mathbf{D}~\\epsilon~\\mathbb{R}^{5000\\times784}$ but has known dimension $5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.regularizers import Regularizer\n",
    "\n",
    "import scipy.sparse\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iPython notebook code is only being provided for convenience.  \n",
    "\n",
    "All Linux code along with scripts is available at https://github.com/nitishbahadur/book_chapter. Our Linux code is based on tensorflow 1.x.  Python package requirements were exported and available https://github.com/nitishbahadur/book_chapter/blob/master/src/requirements.txt.\n",
    "\n",
    "We run our production code on https://arc.wpi.edu/cluster-documentation/build/html/clusters.html for performance reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.get_default_graph()\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The synthetic data is loaded from data folder.  The large numeric number indicates in millisecond when this process was run.  $20$ is the linear dimension estimated using PCA.  We treat the first $3000$ instances as training dataset and remaining $2000$ instances as test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synthetic_data():\n",
    "    X = np.load(r'../data/synthetic_data/input/D_{}_dim_{}.npy'.format(1586038282841, 20), allow_pickle=False)\n",
    "    X = X.astype('float32')\n",
    "\n",
    "    X = X / (np.max(X) - np.min(X))\n",
    "    x_train = X[0:3000,:]\n",
    "    x_test = X[3000:,:]\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the autoencoder model where the innermost layer is using a sigmoid activation function.  The autoencoder also uses dropout layers to control for overfitting.  We use a custom loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lite_ae_model(l1_reg, encoding_dim, layer1_dropout, layer2_dropout):\n",
    "    input_img = Input(shape=(784,))\n",
    "    encoded = Dense(392, activation='relu')(input_img)\n",
    "    encoded = Dropout(layer1_dropout)(encoded)\n",
    "    encoded = Dense(128, activation='relu')(encoded)\n",
    "    encoded = Dropout(layer2_dropout)(encoded)\n",
    "\n",
    "    z_layer_input = Lambda(lambda  x: K.l2_normalize(x,axis=1))(encoded)\n",
    "    encoded = Dense(encoding_dim, activation='sigmoid')(z_layer_input)\n",
    "    encoded_norm = Lambda(lambda  x: K.l2_normalize(x,axis=1))(encoded)\n",
    "    \n",
    "    decoded = Dense(128, activation='relu')(encoded)\n",
    "    decoded = Dense(392, activation='relu')(decoded)\n",
    "    decoded = Dense(784, activation='tanh')(decoded)\n",
    "\n",
    "    # create autoencoder\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "\n",
    "    # create encoder\n",
    "    encoder = Model(input_img, encoded)\n",
    "\n",
    "    # create decoder model\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    deco = autoencoder.layers[-3](encoded_input)\n",
    "    deco = autoencoder.layers[-2](deco)\n",
    "    deco = autoencoder.layers[-1](deco)\n",
    "    decoder = Model(encoded_input, deco)    \n",
    "\n",
    "    autoencoder.compile(optimizer='adadelta', loss=mse_regularized_loss(encoded_norm, l1_reg)) \n",
    "    return encoder, decoder, autoencoder\n",
    "\n",
    "def mse_regularized_loss(encoded_layer, lambda_):    \n",
    "    def loss(y_true, y_pred):\n",
    "        return K.mean(K.square(y_pred - y_true) + lambda_ * K.sum(K.abs(encoded_layer)))\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility functions provided below is equivalent to the python code we use on HPC cluster.  We provide this for completeness here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(encoding_dim, l1_reg, autoencoder, encoder, decoder):\n",
    "    autoencoder_model_path = r\"../data/synthetic_data/output/autoencoder_l1_reg_{}_{}.h5\".format(encoding_dim, l1_reg)\n",
    "    encoder_model_path = r\"../data/synthetic_data/output/encoder_l1_reg_{}_{}.h5\".format(encoding_dim, l1_reg)\n",
    "    decoder_model_path = r\"../data/synthetic_data/output/decoder_l1_reg_{}_{}.h5\".format(encoding_dim, l1_reg)\n",
    "\n",
    "    autoencoder.save(autoencoder_model_path)\n",
    "    print(\"autoencoder saved!!!\")\n",
    "\n",
    "    encoder.save(encoder_model_path) \n",
    "    print(\"encoder saved!!!\")\n",
    "\n",
    "    decoder.save(decoder_model_path) \n",
    "    print(\"decoder saved!!!\")\n",
    "\n",
    "def save_history(encoding_dim, l1_reg, history):\n",
    "    history_filename = r\"../data/synthetic_data/output/history_l1_{}_{}\".format(encoding_dim, l1_reg)\n",
    "    with open(history_filename, 'w') as f:\n",
    "        json.dump(history.history, f)\n",
    "\n",
    "def save_intermediate_training(x, encoder, decoder, epoch):\n",
    "    input_type = 'train'\n",
    "    x_encoded = encoder.predict(x)\n",
    "    x_reconstructed = decoder.predict(x_encoded)\n",
    "\n",
    "    x_encoded_filename = r\"../data/synthetic_data/output/x_{}_{}_encoded_{}_{}\"\n",
    "    np.save(x_encoded_filename.format(input_type, epoch, encoding_dim, l1_reg), x_encoded)\n",
    "\n",
    "    x_reconstructed_filename = r\"../data/synthetic_data/output/x_{}_{}_reconstructed_{}_{}\"\n",
    "    np.save(x_reconstructed_filename.format(input_type, epoch, encoding_dim, l1_reg), x_reconstructed)\n",
    "\n",
    "\n",
    "def save_output(x, autoencoder, encoder, decoder, layer1_dropout, layer2_dropout, input_type):\n",
    "    print(\"{} Original : \".format(input_type))\n",
    "    print(x)\n",
    "\n",
    "    print(\"{} Predicted : \".format(input_type))\n",
    "    x_predicted = autoencoder.predict(x)\n",
    "    print(x_predicted)\n",
    "\n",
    "    print(\"{} Original->Encoded->Decoded(Reconsturcted) : \".format(input_type))\n",
    "    x_encoded = encoder.predict(x)\n",
    "    x_reconstructed = decoder.predict(x_encoded)\n",
    "    print(x_reconstructed)\n",
    "\n",
    "    print(\"{} Encoded : \".format(input_type))\n",
    "    print(x_encoded)    \n",
    "\n",
    "    x_filename = r\"../data/synthetic_data/output/x_{}_{}_{}_{}_{}\"\n",
    "    np.save(x_filename.format(input_type, encoding_dim, l1_reg, layer1_dropout, layer2_dropout), x)\n",
    "\n",
    "    x_encoded_filename = r\"../data/synthetic_data/output/x_{}_encoded_{}_{}_{}_{}\"\n",
    "    np.save(x_encoded_filename.format(input_type, encoding_dim, l1_reg, layer1_dropout, layer2_dropout), x_encoded)\n",
    "\n",
    "    x_predicted_filename = r\"../data/synthetic_data/output/x_{}_predicted_{}_{}_{}_{}\"\n",
    "    np.save(x_predicted_filename.format(input_type, encoding_dim, l1_reg, layer1_dropout, layer2_dropout), x_predicted)\n",
    "\n",
    "class SaveIntermediateTrainingOutput(Callback):\n",
    "    def __init__(self, x, encoder, decoder):\n",
    "        super(Callback, self).__init__()\n",
    "        self.x = x\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.counter = 1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"File counter: {}\".format(self.counter*(epoch+1)))\n",
    "            save_intermediate_training(self.x, self.encoder, self.decoder, self.counter*(epoch+1))\n",
    "            self.counter = self.counter + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate dimension by counting how many singular value proxies are greater than 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_gt_threshold(z, threshold):\n",
    "    tot = sum(z)\n",
    "    z_pct = [(i/tot) for i in sorted(z, reverse=True)]\n",
    "    z_gt_theta = [i for i in z_pct if i >= threshold]\n",
    "    return len(z_gt_theta)\n",
    "\n",
    "def sort_by_row(z):\n",
    "    z_sorted = None\n",
    "    for i in np.arange(z.shape[0]):\n",
    "        z_s = sorted(z[i,:], reverse=True)\n",
    "        if z_sorted is None:\n",
    "            z_sorted = z_s\n",
    "        else:\n",
    "            z_sorted = np.vstack((z_sorted,z_s))\n",
    "    return z_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience we provide default values from run_synthetic_de.py script.  The script is used to run DE process on High Performance Computing cluster at WPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparsity parameter\n",
    "l1_reg = 1.1e-2\n",
    "\n",
    "# number of nodes in innermost hidden layer\n",
    "encoding_dim = 16\n",
    "\n",
    "# number of times you want to run 100 epochs\n",
    "# DE converges slowly.\n",
    "num_epochs = 30\n",
    "\n",
    "# the batch size\n",
    "batch_size = 64\n",
    "\n",
    "# 30% of nodes are dropped out\n",
    "layer1_dropout = 0.3\n",
    "\n",
    "# 30% of nodes are dropped out\n",
    "layer2_dropout = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = get_synthetic_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the encoder, decoder, and autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder, autoencoder = build_lite_ae_model(l1_reg, encoding_dim, layer1_dropout, layer2_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE,100,2.7816,16,16\n",
      "0.6259,0.6203,0.5744,0.5638,0.5514,0.5344,0.5181,0.4979,0.4895,0.4572,0.4375,0.4268,0.4252,0.3863,0.3641,0.3258\n",
      "\n",
      "AE,200,2.7595,16,16\n",
      "0.6646,0.6497,0.5931,0.5839,0.5625,0.5354,0.5306,0.493,0.4694,0.4308,0.4199,0.3925,0.3923,0.3693,0.3178,0.2994\n",
      "\n",
      "AE,300,2.7303,16,16\n",
      "0.6987,0.6775,0.6153,0.6074,0.5779,0.5449,0.5443,0.4863,0.4524,0.3995,0.3919,0.357,0.3553,0.3438,0.2778,0.2694\n",
      "\n",
      "AE,400,2.6887,16,16\n",
      "0.7331,0.7073,0.6431,0.6337,0.5951,0.5567,0.5555,0.4749,0.4275,0.3622,0.3571,0.3152,0.3132,0.3116,0.2385,0.2378\n",
      "\n",
      "AE,500,2.6327,16,16\n",
      "0.7662,0.7383,0.6718,0.6628,0.6137,0.5682,0.5655,0.4545,0.3931,0.3179,0.3158,0.2752,0.2691,0.2677,0.2074,0.2008\n",
      "\n",
      "AE,600,2.5649,16,16\n",
      "0.7964,0.7678,0.7013,0.6921,0.6344,0.5782,0.5739,0.4247,0.3486,0.2712,0.2702,0.2363,0.2248,0.2238,0.1779,0.1667\n",
      "\n",
      "AE,700,2.4875,16,16\n",
      "0.8221,0.7938,0.7293,0.7209,0.6559,0.5852,0.5794,0.3824,0.2974,0.2269,0.2237,0.1986,0.1845,0.1839,0.1513,0.1374\n",
      "\n",
      "AE,800,2.4062,16,16\n",
      "0.8441,0.8168,0.7548,0.7477,0.6761,0.587,0.5797,0.33,0.2455,0.1868,0.1822,0.1656,0.1505,0.1501,0.1286,0.1136\n",
      "\n",
      "AE,900,2.3272,16,16\n",
      "0.8621,0.8364,0.7773,0.7717,0.6943,0.5818,0.5729,0.2734,0.1984,0.1528,0.1477,0.1377,0.1231,0.1229,0.1095,0.0947\n",
      "\n",
      "AE,1000,2.2541,16,16\n",
      "0.8767,0.8527,0.7971,0.7927,0.7098,0.5679,0.5571,0.2201,0.1594,0.1255,0.1204,0.1148,0.1015,0.1015,0.0936,0.0797\n",
      "\n",
      "AE,1100,2.1891,16,16\n",
      "0.8887,0.8666,0.8143,0.8109,0.7228,0.5436,0.5303,0.1752,0.1286,0.1039,0.0991,0.0963,0.0847,0.0847,0.0806,0.0679\n",
      "\n",
      "AE,1200,2.1308,15,15\n",
      "0.8985,0.8782,0.8292,0.8268,0.7334,0.5069,0.4909,0.1398,0.1048,0.0871,0.0827,0.0814,0.0717,0.0717,0.0699,0.0585\n",
      "\n",
      "AE,1300,2.0779,15,15\n",
      "0.9067,0.8882,0.8422,0.8406,0.7419,0.4568,0.4382,0.1129,0.0867,0.0741,0.07,0.0696,0.0615,0.0615,0.0613,0.0511\n",
      "\n",
      "AE,1400,2.0261,12,12\n",
      "0.9136,0.8967,0.8535,0.8528,0.7488,0.3949,0.3749,0.0925,0.0729,0.0637,0.0601,0.06,0.0541,0.0534,0.0534,0.0452\n",
      "\n",
      "AE,1500,1.9751,10,10\n",
      "0.9193,0.904,0.8633,0.8633,0.7545,0.3273,0.3078,0.0771,0.0621,0.0554,0.0524,0.0523,0.0481,0.0469,0.0468,0.0403\n",
      "\n",
      "AE,1600,1.9239,9,9\n",
      "0.9241,0.9101,0.8722,0.8716,0.7585,0.2621,0.2449,0.0651,0.0536,0.0485,0.0461,0.0458,0.0429,0.0414,0.0414,0.0361\n",
      "\n",
      "AE,1700,1.8768,8,8\n",
      "0.9282,0.9153,0.8798,0.8788,0.7607,0.2064,0.1924,0.0557,0.0468,0.0427,0.0408,0.0405,0.0383,0.0369,0.0369,0.0325\n",
      "\n",
      "AE,1800,1.8366,7,7\n",
      "0.9318,0.9197,0.8862,0.885,0.7609,0.1626,0.1515,0.0483,0.0412,0.0379,0.0365,0.0361,0.0344,0.0331,0.033,0.0294\n",
      "\n",
      "AE,1900,1.8028,7,7\n",
      "0.9348,0.9235,0.8916,0.8903,0.7587,0.1294,0.1208,0.0423,0.0366,0.0339,0.0328,0.0323,0.0309,0.0298,0.0298,0.0267\n",
      "\n",
      "AE,2000,1.7748,7,7\n",
      "0.9375,0.9268,0.8962,0.8949,0.7542,0.1047,0.0979,0.0375,0.0327,0.0305,0.0296,0.0292,0.028,0.027,0.027,0.0244\n",
      "\n",
      "AE,2100,1.7511,7,7\n",
      "0.9398,0.9297,0.9002,0.8988,0.7469,0.0861,0.0808,0.0335,0.0295,0.0276,0.027,0.0265,0.0255,0.0246,0.0246,0.0223\n",
      "\n",
      "AE,2200,1.7316,7,7\n",
      "0.9418,0.932,0.9034,0.9025,0.7359,0.0719,0.0677,0.0302,0.0268,0.0251,0.0246,0.0242,0.0232,0.0226,0.0226,0.0206\n",
      "\n",
      "AE,2300,1.7145,7,7\n",
      "0.9435,0.9341,0.9061,0.9056,0.7213,0.0608,0.0574,0.0274,0.0243,0.023,0.0227,0.0223,0.0213,0.021,0.0208,0.0191\n",
      "\n",
      "AE,2400,1.7004,7,7\n",
      "0.945,0.9361,0.909,0.9084,0.7021,0.0524,0.0497,0.025,0.0224,0.0212,0.021,0.0206,0.0196,0.0194,0.0193,0.0178\n",
      "\n",
      "AE,2500,1.6875,5,5\n",
      "0.9463,0.9379,0.9116,0.9109,0.6764,0.0457,0.0435,0.0231,0.0207,0.0196,0.0195,0.0191,0.0182,0.018,0.018,0.0167\n",
      "\n",
      "AE,2600,1.6753,5,5\n",
      "0.9474,0.9396,0.9141,0.9132,0.6418,0.0403,0.0384,0.0213,0.0193,0.0182,0.0182,0.0178,0.017,0.0169,0.0168,0.0157\n",
      "\n",
      "AE,2700,1.6633,5,5\n",
      "0.9482,0.9411,0.9163,0.9153,0.595,0.0359,0.0343,0.0199,0.018,0.0171,0.017,0.0167,0.0159,0.0159,0.0158,0.0149\n",
      "\n",
      "AE,2800,1.6497,5,5\n",
      "0.9487,0.9423,0.9184,0.9172,0.5326,0.0323,0.031,0.0187,0.017,0.0162,0.016,0.0157,0.0151,0.015,0.0149,0.0142\n",
      "\n",
      "AE,2900,1.6337,5,5\n",
      "0.9489,0.9433,0.9201,0.9187,0.454,0.0294,0.0282,0.0177,0.0162,0.0155,0.0151,0.015,0.0145,0.0143,0.0142,0.0137\n",
      "\n",
      "AE,3000,1.6140,5,5\n",
      "0.949,0.944,0.9217,0.92,0.3667,0.0269,0.0259,0.0169,0.0155,0.0149,0.0144,0.0143,0.0139,0.0137,0.0136,0.0133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svp_dict_ = {}\n",
    "dim_dict_ = {}\n",
    "for i in range(1, num_epochs+1):\n",
    "        history = autoencoder.fit(x_train, x_train, epochs=100, batch_size=batch_size, verbose=0)\n",
    "        z = encoder.predict(x_test)\n",
    "        z_row_sorted = sort_by_row(z)\n",
    "        z_mu = np.mean(z_row_sorted, axis=0)\n",
    "        gte_sorted = count_gt_threshold(z_mu, 0.01)\n",
    "        \n",
    "        z_mu_1 = sorted(np.mean(z, axis=0), reverse=True)\n",
    "        gte_dim = count_gt_threshold(z_mu_1, 0.01)\n",
    "        loss = history.history['loss'][-1]\n",
    "        print(\"AE,{},{:.4f},{},{}\".format(i*100, loss, gte_sorted, gte_dim))\n",
    "        \n",
    "        converted_list = [str(np.round(element, 4)) for element in z_mu_1]\n",
    "        svps = \",\".join(converted_list)    \n",
    "        print(svps)\n",
    "        print()\n",
    "        \n",
    "        # save it for plotting later\n",
    "        svp_dict_[i*100] = svps\n",
    "        dim_dict_[i*100] = gte_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The singular value proxy keeps reducing.  We take a snapshot every 100 epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythongpu",
   "language": "python",
   "name": "pythongpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
